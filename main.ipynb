{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aed247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "LANGCHAIN_ENDPOINT = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341871e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task Decomposition involves breaking down a complex task into smaller, manageable steps or subgoals. Techniques like Chain of Thought and Tree of Thoughts facilitate this process by generating step-by-step reasoning or multiple reasoning paths. It can be performed by language models through prompting, using task-specific instructions, or with external planning tools.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "## Indexing\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "## Retrieval and Generation\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0)\n",
    "\n",
    "# post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed2e0a",
   "metadata": {},
   "source": [
    "### Part 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f551f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5fe5e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb96121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed = OpenAIEmbeddings()\n",
    "query_result = embed.embed_query(question)\n",
    "document_result = embed.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2771efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8806977856520077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c9fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11191fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5ca496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be700ec2",
   "metadata": {},
   "source": [
    "### Part 3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e8dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})    # Number of nearby neighbors to fetch in the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c880a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_11568\\1542214506.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52294d",
   "metadata": {},
   "source": [
    "### Part 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e71c1925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e021ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e000949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd5e8233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps or subgoals. Techniques for task decomposition include prompting a large language model (LLM) with simple instructions or questions, using task-specific instructions (e.g., \"Write a story outline\"), or incorporating human inputs. This approach helps in planning and solving complicated tasks by simplifying them into easier components.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 317, 'total_tokens': 395, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BuXT32edkJt9zjrYr4p2OA9VGOHIw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--aebdb2ec-6858-45c7-a123-e920283f1dee-0', usage_metadata={'input_tokens': 317, 'output_tokens': 78, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": \"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ad2d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a9cd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ac1de43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down a complex task into smaller, more manageable steps or subgoals. Techniques for task decomposition include prompting large language models with simple instructions (e.g., listing steps or subgoals), using task-specific instructions (e.g., \"Write a story outline\"), or incorporating human inputs. This approach helps in planning and solving complicated tasks by transforming them into simpler components.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b213f",
   "metadata": {},
   "source": [
    "### Part 5: Multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcbf50e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# Load blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# Index\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe300fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "## Prompt\n",
    "\n",
    "# Multi-query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86231652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_12440\\839454571.py:10: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\"Unique union of retrieved docs\"\"\"\n",
    "    # Flatten list of lists, and convert each document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is Task Decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c27d13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition for LLM agents involves breaking down complex tasks into smaller, manageable subgoals or steps to facilitate efficient problem-solving. This process can be achieved through various methods:\\n\\n1. **Prompting Techniques:** Using simple prompts like \"Steps for XYZ\" or asking \"What are the subgoals for achieving XYZ?\" to guide the LLM in identifying smaller components of the main task.\\n\\n2. **Task-Specific Instructions:** Providing specific instructions tailored to the task, such as \"Write a story outline,\" to help the model focus on particular aspects of the task.\\n\\n3. **Human Inputs:** Incorporating human guidance or oversight to assist in decomposing tasks effectively.\\n\\nAdditionally, advanced methods like **Chain of Thought (CoT)** prompting encourage the model to think step-by-step, while **Tree of Thoughts** extends this by exploring multiple reasoning paths simultaneously, generating a tree structure of possible solutions. These approaches enable the LLM to interpret complex problems more effectively by systematically breaking them into simpler, sequential steps or multiple reasoning pathways.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009204f",
   "metadata": {},
   "source": [
    "### Part 6: RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "139961ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d28230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13ead9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_29076\\3246853674.py:26: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  (loads(doc), score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import loads, dumps\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "        and an optional parameter k used in the RRF formula\"\"\"\n",
    "        \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "    \n",
    "    # iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    \n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    \n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "question = \"What is Task Decomposition for LLM agents?\"\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8d80d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition for LLM agents involves breaking down complex tasks into smaller, manageable subgoals or steps to facilitate efficient problem-solving. This process can be achieved through various methods:\\n\\n1. Using simple prompting techniques with the language model, such as asking \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to guide the model in identifying smaller tasks.\\n2. Employing task-specific instructions tailored to the nature of the task, for example, instructing the model to \"Write a story outline\" for creative writing.\\n3. Incorporating human inputs to assist in decomposing tasks when necessary.\\n\\nAdditionally, advanced methods like the Tree of Thoughts extend this concept by exploring multiple reasoning paths at each step, generating a tree structure of possible solutions, and evaluating them through classifiers or majority voting. Overall, task decomposition helps transform complex problems into simpler components, enabling better planning and reasoning in LLM-powered agents.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a2722",
   "metadata": {},
   "source": [
    "### Part 7: Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b614b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ceab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314f2b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the key architectural components of an autonomous agent system powered by large language models (LLMs)?  ',\n",
       " '2. How do perception, reasoning, and action modules integrate within an LLM-based autonomous agent?  ',\n",
       " '3. What infrastructure and data components are essential for deploying and maintaining an LLM-driven autonomous agent system?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a86939cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\\n -- \\n {question} \\n -- \\n\n",
    "Here is any available background question + answer pairs:\n",
    "\\n -- \\n {q_a_pairs} \\n -- \\n\n",
    "Here is additional context relevant to the question:\n",
    "\\n -- \\n {context} \\n -- \\n\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f23bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Formay Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser())\n",
    "        \n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n--\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d1e287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To effectively deploy and maintain an LLM-driven autonomous agent system, several key infrastructure and data components are essential. These components ensure the system's robustness, scalability, responsiveness, and continual improvement. Based on the provided context and architectural insights, the essential components include:\\n\\n1. **Computational Infrastructure:**  \\n   - **High-Performance Servers or Cloud Platforms:** To host large language models (LLMs) and support intensive processing tasks. Cloud providers (e.g., AWS, Azure, GCP) offer scalable resources for model deployment, inference, and training.  \\n   - **GPU/TPU Resources:** Specialized hardware accelerators are often necessary for efficient inference and fine-tuning of large models.\\n\\n2. **Model Hosting and Serving Infrastructure:**  \\n   - **Model Deployment Frameworks:** Tools like TensorFlow Serving, TorchServe, or custom APIs facilitate scalable, low-latency access to LLMs.  \\n   - **API Gateways and Load Balancers:** Manage incoming requests, distribute load, and ensure high availability.\\n\\n3. **Data Storage and Management Systems:**  \\n   - **Memory and Context Storage:** Persistent storage (databases, vector stores, or key-value stores) to maintain historical interactions, past actions, and contextual information, enabling the agent to reason over long-term data.  \\n   - **Knowledge Bases and External Data Sources:** Databases, knowledge graphs, or APIs that provide external information to augment the LLM’s capabilities.\\n\\n4. **Perception and Input Processing Components:**  \\n   - **Data Ingestion Pipelines:** Systems to collect, preprocess, and convert raw inputs (user prompts, sensor data, environmental signals) into structured formats suitable for the LLM.  \\n   - **Interfaces for External Data:** APIs or connectors to external knowledge sources, search engines, or tools.\\n\\n5. **Action Execution Environment:**  \\n   - **API and Tool Integration:** Interfaces to execute actions such as querying databases, calling external APIs, or controlling hardware (e.g., robots).  \\n   - **Execution Orchestration:** Systems to manage and coordinate actions based on the reasoning outputs.\\n\\n6. **Monitoring and Feedback Systems:**  \\n   - **Performance Monitoring:** Tools to track system health, response times, and accuracy.  \\n   - **Logging and Analytics:** To record interactions, decisions, and outcomes for analysis and continuous improvement.  \\n   - **Feedback Loops:** Mechanisms to incorporate feedback from environment or users to refine models and system behavior.\\n\\n7. **Development and Maintenance Tools:**  \\n   - **Version Control and CI/CD Pipelines:** For deploying updates, managing model versions, and ensuring system stability.  \\n   - **Reflection and Self-Improvement Modules:** Components that enable the agent to perform self-criticism, reflection, and iterative refinement based on past performance data.\\n\\n**In summary:**  \\nDeploying and maintaining an LLM-driven autonomous agent requires a robust computational infrastructure (cloud servers, accelerators), scalable model hosting and serving systems, persistent storage for memory and knowledge, data ingestion pipelines, external tool and API interfaces, monitoring and feedback mechanisms, and development tools for ongoing updates. These components work together to support the agent’s perception, reasoning, action execution, and continuous learning, ensuring reliable and effective autonomous operation.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4de9b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_7300\\1526751718.py:18: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
     ]
    }
   ],
   "source": [
    "# Answer each sub-question individually\n",
    "from langchain import hub\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generate_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition\n",
    "    sub_questions = sub_question_generate_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "        \n",
    "        rag_results.append(answer)\n",
    "        \n",
    "    return rag_results, sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a runnablelambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9da8a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main components of an LLM-powered autonomous agent system include the large language model (LLM) itself, which acts as the core decision-maker; planning modules responsible for decomposing tasks and enabling reflection; perception modules that gather environmental information; action modules that facilitate interaction with external tools or environments; memory components that store past actions, knowledge, and reflections; and supporting infrastructure such as computational resources and APIs to ensure reliable operation and scalability. Together, these components enable the system to perceive, reason, plan, act, reflect, and learn continuously.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "{context}\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "final_rag_chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193fe4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
